{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.023479,"end_time":"2021-07-16T17:41:26.264343","exception":false,"start_time":"2021-07-16T17:41:26.240864","status":"completed"},"tags":[]},"source":["<h1>Introduction to Information Retrieval - <i>Machado de Assis Collection</i></h1> \n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.022339,"end_time":"2021-07-16T17:41:26.309804","exception":false,"start_time":"2021-07-16T17:41:26.287465","status":"completed"},"tags":[]},"source":["Disclaimer: this tutorial follows the ideas presented in [Stanford CS124 class Week 4](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA) and in Chis Manning's Book [An Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf). Also, some code was borrowed from this [github repo](https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb).\n","\n","In this tutorial we'll cover the basics of Information Retrieval (IR) concepts and focus on Boolean and TF-IDF Ranked Retrieval models. At the end we present ways to evaluate an IR system using a benchmark dataset and an algorithm shipped with modern search engines based on Lucene (i.e. Elasticsearch and Solr).\n","\n","For our demo, we'll use a collection of [Machado de Assis](https://pt.wikipedia.org/wiki/Machado_de_Assis)'s books and articles. He is a famous brazilian writer. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-08-23T23:34:37.542069Z","iopub.status.busy":"2021-08-23T23:34:37.541695Z","iopub.status.idle":"2021-08-23T23:34:39.227564Z","shell.execute_reply":"2021-08-23T23:34:39.226298Z","shell.execute_reply.started":"2021-08-23T23:34:37.542035Z"},"papermill":{"duration":1.752359,"end_time":"2021-07-16T17:41:28.084744","exception":false,"start_time":"2021-07-16T17:41:26.332385","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Library imports\n","import numpy as np \n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import os, glob, re, sys, random, unicodedata, collections\n","from tqdm import tqdm\n","from functools import reduce\n","import nltk\n","from collections import Counter\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","from nltk.tokenize import sent_tokenize , word_tokenize\n","\n","STOP_WORDS = set(stopwords.words('portuguese'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.024211,"end_time":"2021-07-16T17:41:28.312559","exception":false,"start_time":"2021-07-16T17:41:28.288348","status":"completed"},"tags":[]},"source":["# 1. Boolean Retrieval Model\n","\n","We'll start this tutorial with the boolean retrieval model."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.1 Basic Concepts"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.022924,"end_time":"2021-07-16T17:41:28.406578","exception":false,"start_time":"2021-07-16T17:41:28.383654","status":"completed"},"tags":[]},"source":["[Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval) could be defined as a field of study interested in ways to finding relevant material of an unstructured nature (usually text) from a collection of data (i.e.: files stored in your file system) to satisfy user information need. \n","\n","Users try to translate their needs into a *query*. This *query* is processed by a *search engine* over the *collection* and retrive matching *results*. Users then evaluates the relevance of these *results* and refine his *query* iteratively.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.2 Term-document frequency\n","\n","An important processing step of a *search engine* is to build a term-document frequency table, counting the number of occurrences (or a boolean version) each term (or word) occurs in each document (or file). \n","\n","We illustrate this in the piece of code below. For simplification, we use a scikit-learn builtin function [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to achieve this.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:34:39.229699Z","iopub.status.busy":"2021-08-23T23:34:39.229356Z","iopub.status.idle":"2021-08-23T23:34:39.543231Z","shell.execute_reply":"2021-08-23T23:34:39.542225Z","shell.execute_reply.started":"2021-08-23T23:34:39.229665Z"},"papermill":{"duration":0.111819,"end_time":"2021-07-16T17:41:28.264925","exception":false,"start_time":"2021-07-16T17:41:28.153106","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["random.seed(42) ## turn it reproductible\n","\n","files = []\n","# Read only the txt version of files and discard the pdf files of the colleciton\n","for dirname, _, filenames in os.walk('/kaggle/input/machado-de-assis/raw/txt/'):\n","    for filename in filenames:\n","        files.append(os.path.join(dirname, filename))\n","\n","print('There are a total of {} files'.format(len(files)), '\\n')\n","        \n","# select and read 10 random files \n","sample_books = random.sample(files,10)\n","\n","docs = []\n","for fname in sample_books:\n","    with open(fname , \"r\") as file:\n","        text = file.read()\n","    docs.append(text)\n","\n","# count term frequency using CountVectorizer from scikit-learn\n","## limiting number of words just for illustrating the concept\n","vec = CountVectorizer(max_features=10, stop_words=STOP_WORDS) \n","X = vec.fit_transform(docs)\n","df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n","books_names = [book.split('/')[-1] for book in sample_books]\n","df['book'] = books_names\n","df = df.set_index('book')\n","\n","print(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The table above shows 10 words occurrence into 10 books sampled  from the 116 available. Now, from this table, we're able to search documents which contains specific terms using set operations (AND, OR, NOT). \n","\n","For example, for table above, if our *query* is the *term* \"helena\", documents number 2, 3 and 9 match our criteria. If our *query* is \"estácio\", only document 9 matches. If our *query* is \"helena AND estácio\", the engine will compute (2, 3, 9) AND (9) and return 9. Also, with a *query* \"helena OR estácio\", it will compute (2, 3, 9) OR (9) and return (2, 3, 9) as results. (Remember: OR is Union and AND is Intersection)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["One problem with this approach is, since a big collection usually has hundreds of thousands of distinct words and possibly another hundreds of thousands of distinct documents, this table (or matrix) will have approxemately 10^10 elements, or 10 Billion. Even using a efficient data structure for sparse data, it's hard to build such matrix. \n","\n","One possible solution is to store the information in a inverted manner: An <b>Inverted Index</b>.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.3 Inverted Index"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.022943,"end_time":"2021-07-16T17:41:28.499003","exception":false,"start_time":"2021-07-16T17:41:28.47606","status":"completed"},"tags":[]},"source":["### 1.3.1 Cleanning up and tokenizing text\n","\n","In this section we'll tokenize (split it into terms or tokens) a piece of text and clean it: eliminate accentuation, puctuation and put all letter into lower case. We also remove stop words, the most common words such as articles, prepositions, etc.\n","These steps are important to improve the results of our search engine, because since a user can search by \"Capitú\", \"capitu\", \"Capitu\" or \"capitú\", we want all these forms to match te character of Machado de Assis's book, \"Dom Casmurro\".  So we process both the text of the books and later the user queries with same methods."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:34:39.545422Z","iopub.status.busy":"2021-08-23T23:34:39.545057Z","iopub.status.idle":"2021-08-23T23:34:39.554236Z","shell.execute_reply":"2021-08-23T23:34:39.553052Z","shell.execute_reply.started":"2021-08-23T23:34:39.545387Z"},"papermill":{"duration":0.041411,"end_time":"2021-07-16T17:41:28.564565","exception":false,"start_time":"2021-07-16T17:41:28.523154","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["WORD_MIN_LENGTH = 2 ## we'll drop all tokens with less than this size\n","\n","def strip_accents(text):\n","    \"\"\"Strip accents and punctuation from text. \n","    For instance: strip_accents(\"João e Maria, não entrem!\") \n","    will return \"Joao e Maria  nao entrem \"\n","\n","    Parameters:\n","    text (str): Input text\n","\n","    Returns:\n","    str: text without accents and punctuation\n","\n","   \"\"\"    \n","    nfkd = unicodedata.normalize('NFKD', text)\n","    newText = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n","    return re.sub('[^a-zA-Z0-9 \\\\\\']', ' ', newText)\n","\n","def tokenize_text(text):\n","    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, \n","    remove \\n, tokenize our text, convert to lower case, remove stop words and \n","    words with less than 2 chars.\n","\n","    Parameters:\n","    text (str): Input text\n","\n","    Returns:\n","    str: cleaned tokenized text\n","\n","   \"\"\"        \n","    text = strip_accents(text)\n","    text = re.sub(re.compile('\\n'),' ',text)\n","    words = word_tokenize(text)\n","    words = [word.lower() for word in words]\n","    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n","    return words"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.3.2 Building Inverted index"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To build our Inverted Index, we'll use two auxiliary functions. The first one process the tokenized text from one document (book) to build a local inverted index. The second merge this information into our Inverted Index adding document id of each document. The final structure of our index will have the form of:\n","\n","<pre>\n","{'term1': \n","    {doc_id0: [pos0, pos1, pos2], #positions of term1 found in doc_id0\n","    doc_id1: [pos0, pos1, pos2] #positions of term1 found in doc_id1\n","}, 'term2': \n","    {doc_id0: [pos0, pos1, pos2], #positions of term1 found in doc_id0\n","    doc_id3: [pos0, pos1, pos2] #positions of term1 found in doc_id3\n","    ...\n","}\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:34:39.556524Z","iopub.status.busy":"2021-08-23T23:34:39.55612Z","iopub.status.idle":"2021-08-23T23:34:39.569915Z","shell.execute_reply":"2021-08-23T23:34:39.56869Z","shell.execute_reply.started":"2021-08-23T23:34:39.556491Z"},"papermill":{"duration":0.033021,"end_time":"2021-07-16T17:41:28.620725","exception":false,"start_time":"2021-07-16T17:41:28.587704","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def inverted_index(words):\n","    \"\"\"Create a inverted index of words (tokens or terms) from a list of terms\n","\n","    Parameters:\n","    words (list of str): tokenized document text\n","\n","    Returns:\n","    Inverted index of document (dict)\n","\n","   \"\"\"       \n","    inverted = {}\n","    for index, word in enumerate(words):\n","        locations = inverted.setdefault(word, [])\n","        locations.append(index)\n","    return inverted\n","\n","def inverted_index_add(inverted, doc_id, doc_index):\n","    \"\"\"Insert document id into Inverted Index\n","\n","    Parameters:\n","    inverted (dict): Inverted Index\n","    doc_id (int): Id of document been added\n","    doc_index (dict): Inverted Index of a specific document.\n","\n","    Returns:\n","    Inverted index of document (dict)\n","\n","   \"\"\"        \n","    for word in doc_index.keys():\n","        locations = doc_index[word]\n","        indices = inverted.setdefault(word, {})\n","        indices[doc_id] = locations\n","    return inverted"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we iterate over our collection of books and build our Inverted Index."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:34:39.571924Z","iopub.status.busy":"2021-08-23T23:34:39.571599Z","iopub.status.idle":"2021-08-23T23:35:00.919438Z","shell.execute_reply":"2021-08-23T23:35:00.918391Z","shell.execute_reply.started":"2021-08-23T23:34:39.571895Z"},"papermill":{"duration":21.406144,"end_time":"2021-07-16T17:41:50.050115","exception":false,"start_time":"2021-07-16T17:41:28.643971","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["inverted_doc_indexes = {}\n","files_with_index = []\n","files_with_tokens = {}\n","doc_id=0\n","for fname in tqdm(files):\n","    with open(fname , \"r\") as file:\n","        text = file.read()\n","    #Clean and Tokenize text of each document\n","    words = tokenize_text(text)\n","    #Store tokens\n","    files_with_tokens[doc_id] = words\n","\n","    doc_index = inverted_index(words)\n","    inverted_index_add(inverted_doc_indexes, doc_id, doc_index)\n","    files_with_index.append(os.path.basename(fname))\n","    doc_id = doc_id+1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.921655Z","iopub.status.busy":"2021-08-23T23:35:00.92093Z","iopub.status.idle":"2021-08-23T23:35:00.928318Z","shell.execute_reply":"2021-08-23T23:35:00.927345Z","shell.execute_reply.started":"2021-08-23T23:35:00.92161Z"},"trusted":true},"outputs":[],"source":["## Check presence of capitu token into Dom Casmurro book:\n","capitu_docs = inverted_doc_indexes['capitu']\n","for idx in capitu_docs.keys():\n","    print(files_with_index[idx])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.4 Running boolean search\n","Now we're ready to run boolean search on our inverted index. This index and query processing together consists of we call a *search engine*. To simplify, again, our boolean search function will use only AND operator, retrieving only documents that contain all terms in user query."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.929808Z","iopub.status.busy":"2021-08-23T23:35:00.929528Z","iopub.status.idle":"2021-08-23T23:35:00.9395Z","shell.execute_reply":"2021-08-23T23:35:00.938356Z","shell.execute_reply.started":"2021-08-23T23:35:00.92978Z"},"papermill":{"duration":0.049326,"end_time":"2021-07-16T17:41:50.13902","exception":false,"start_time":"2021-07-16T17:41:50.089694","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Using AND as logical operator\n","def boolean_search(inverted, file_names, query):\n","    \"\"\"Run a boolean search with AND operator between terms over \n","    the inverted index.\n","\n","    Parameters:\n","    inverted (dict): Inverted Index\n","    file_names (list): List with names of files (books)\n","    query (txt): Query text\n","\n","    Returns:\n","    Names of books that matchs the query.\n","\n","   \"\"\"      \n","    # preprocess the user query using same function used to build Inverted Index\n","    words = [word for _, word in enumerate(tokenize_text(query)) if word in inverted]\n","    # list with a disctinct document match for each term from query\n","    results = [set(inverted[word].keys()) for word in words]\n","    # AND operator. Replace & for | to modify to OR behavior.\n","    docs = reduce(lambda x, y: x & y, results) if results else []\n","    return ([file_names[doc] for doc in docs])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we'll test some passages and character names from well known books from author."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.943651Z","iopub.status.busy":"2021-08-23T23:35:00.943036Z","iopub.status.idle":"2021-08-23T23:35:00.957658Z","shell.execute_reply":"2021-08-23T23:35:00.956379Z","shell.execute_reply.started":"2021-08-23T23:35:00.943604Z"},"papermill":{"duration":0.048507,"end_time":"2021-07-16T17:41:50.22678","exception":false,"start_time":"2021-07-16T17:41:50.178273","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Passage from \"Quincas borba\"\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Ao vencido, ódio ou compaixão; ao vencedor, as batatas\"))\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Quincas borba\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.960751Z","iopub.status.busy":"2021-08-23T23:35:00.960262Z","iopub.status.idle":"2021-08-23T23:35:00.970699Z","shell.execute_reply":"2021-08-23T23:35:00.969765Z","shell.execute_reply.started":"2021-08-23T23:35:00.960706Z"},"papermill":{"duration":0.047297,"end_time":"2021-07-16T17:41:50.313737","exception":false,"start_time":"2021-07-16T17:41:50.26644","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Passage from \"Dom Casmurro\"\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Capitu, apesar daqueles olhos que o diabo lhe deu\"))\n","# Passage from \"Memórias Póstumas de Brás Cubas\"\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Capitu\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.97231Z","iopub.status.busy":"2021-08-23T23:35:00.97202Z","iopub.status.idle":"2021-08-23T23:35:00.985451Z","shell.execute_reply":"2021-08-23T23:35:00.98446Z","shell.execute_reply.started":"2021-08-23T23:35:00.972284Z"},"papermill":{"duration":0.049619,"end_time":"2021-07-16T17:41:50.402911","exception":false,"start_time":"2021-07-16T17:41:50.353292","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Passage from \"Memórias Póstumas de Brás Cubas\"\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Sandice criar amor às casas alheias, de modo que, \\\n","                     apenas senhora de uma, dificilmente lha farão despejar\"))\n","print(boolean_search(inverted_doc_indexes, files_with_index, \n","                     \"Sandice\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see above, our boolean search works pretty well. But this model has few problems:\n","\n","* The user must have some knowledge of the collection to proper use the logical operators (OR, AND, NOT...). We over simplified here using only AND.\n","* We don't have a notion of rank here. For example, if we search for a couple of common words not present into STOP_WORDS, it'll probably return all documents, but without any importance order. \n","* Larger documents have bigger probability to be returned in any query, since it contains more terms. \n","\n","We'll try to address these issues with another retrieval model called Ranked TF-IDF.\n","\n","PS: There are several issues related to deal with phrase queries, but we won't deal with them. Please refer to Chapter 2 of Chris Manning's [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:00.987335Z","iopub.status.busy":"2021-08-23T23:35:00.986878Z","iopub.status.idle":"2021-08-23T23:35:00.999346Z","shell.execute_reply":"2021-08-23T23:35:00.998187Z","shell.execute_reply.started":"2021-08-23T23:35:00.987289Z"},"trusted":true},"outputs":[],"source":["## Exaple of two term query present in several books, but there's \n","## no relevance order to evaluate with result to look first. \n","print(boolean_search(inverted_doc_indexes, files_with_index, \"árvore rua\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.03969,"end_time":"2021-07-16T17:41:50.48267","exception":false,"start_time":"2021-07-16T17:41:50.44298","status":"completed"},"tags":[]},"source":["# 2. Ranked TF-IDF Retrieval Model"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.039432,"end_time":"2021-07-16T17:41:50.562115","exception":false,"start_time":"2021-07-16T17:41:50.522683","status":"completed"},"tags":[]},"source":["TF-IDF stands for term frequency–inverse document frequency. It's a retrieval model whereas each pair of term *i* in document *d* receiveis a weight given by formulas below:"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.039571,"end_time":"2021-07-16T17:41:50.64148","exception":false,"start_time":"2021-07-16T17:41:50.601909","status":"completed"},"tags":[]},"source":["* $ \\mbox{Tf-Idf}_{i,d} = \\mbox{Tf}_{i,d} \\cdot \\mbox{Idf}_{i} $\n","\n","* $ \\mbox{Tf}_{i,d} = 1 + log(f_{i,d}) $, where $f_{i,d}$ is how many times term $i$ occurs in document $d$\n","\n","* $ \\mbox{Idf}_{i,d} = log(N / n_{t}) $, where $N$ is the number of documents of Collection and $n_{t}$ is the number of documents the term occurs \n","\n","\n","Then, we compute the relevance of a document to a specific query adding weights of each term of query present in each document:\n","\n","* $ Score_{q,d} = \\sum_{t \\in q \\cap d} \\mbox{Tf-Idf}_{t,d} $\n","\n","\n","See https://en.wikipedia.org/wiki/Tf%E2%80%93idf"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To acomplish this model here, we'll create a data structure using a dictionary whereas the key is the pair (term, doc_id) and value is Tf-Idf weight. First we calculate the term frequency (number of documents each term is present)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:01.00137Z","iopub.status.busy":"2021-08-23T23:35:01.000743Z","iopub.status.idle":"2021-08-23T23:35:01.092945Z","shell.execute_reply":"2021-08-23T23:35:01.092095Z","shell.execute_reply.started":"2021-08-23T23:35:01.001326Z"},"papermill":{"duration":0.130898,"end_time":"2021-07-16T17:41:50.812028","exception":false,"start_time":"2021-07-16T17:41:50.68113","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Number of documents each term occurs\n","DF = {}\n","for word in inverted_doc_indexes.keys():\n","    DF[word] = len ([doc for doc in inverted_doc_indexes[word]])\n","\n","total_vocab_size = len(DF)\n","print(total_vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:01.094472Z","iopub.status.busy":"2021-08-23T23:35:01.094018Z","iopub.status.idle":"2021-08-23T23:35:04.463617Z","shell.execute_reply":"2021-08-23T23:35:04.462551Z","shell.execute_reply.started":"2021-08-23T23:35:01.094441Z"},"papermill":{"duration":3.334856,"end_time":"2021-07-16T17:41:54.187167","exception":false,"start_time":"2021-07-16T17:41:50.852311","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["tf_idf = {} # Our data structure to store Tf-Idf weights\n","\n","N = len(files_with_tokens)\n","\n","for doc_id, tokens in tqdm(files_with_tokens.items()):\n","    \n","    counter = Counter(tokens)\n","    words_count = len(tokens)\n","    \n","    for token in np.unique(tokens):\n","        \n","        # Calculate Tf\n","        tf = counter[token] # Counter returns a tuple with each terms counts\n","        tf = 1+np.log(tf)\n","        \n","        # Calculate Idf\n","        if token in DF:\n","            df = DF[token]\n","        else:\n","            df = 0\n","        idf = np.log((N+1)/(df+1))\n","        \n","        # Calculate Tf-idf        \n","        tf_idf[doc_id, token] = tf*idf"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.1 Running Ranked search\n","\n","Similar to boolean search, here we define a function to process a query and return documents that matches it."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.465385Z","iopub.status.busy":"2021-08-23T23:35:04.465051Z","iopub.status.idle":"2021-08-23T23:35:04.473098Z","shell.execute_reply":"2021-08-23T23:35:04.471668Z","shell.execute_reply.started":"2021-08-23T23:35:04.465355Z"},"papermill":{"duration":0.055187,"end_time":"2021-07-16T17:41:54.28755","exception":false,"start_time":"2021-07-16T17:41:54.232363","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def ranked_search(k, tf_idf_index, file_names, query):\n","    \"\"\"Run ranked query search using tf-idf model.\n","\n","    Parameters:\n","    k (int): number of results to return\n","    tf_idf_index (dict): Data Structure storing Tf-Idf weights to each \n","                        pair of (term,doc_id) \n","    file_names (list): List with names of files (books)\n","    query (txt): Query text\n","\n","    Returns:\n","    Top-k names of books that matchs the query.\n","\n","   \"\"\"   \n","    tokens = tokenize_text(query)\n","    query_weights = {}\n","    for doc_id, token in tf_idf:\n","        if token in tokens:\n","            query_weights[doc_id] = query_weights.get(doc_id, 0) + tf_idf_index[doc_id, token]\n","    \n","    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n","    results = []\n","    for i in query_weights[:k]:\n","        results.append(file_names[i[0]])\n","    \n","    return results\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Again, we'll test same passages and character names we used previously."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.474792Z","iopub.status.busy":"2021-08-23T23:35:04.474509Z","iopub.status.idle":"2021-08-23T23:35:04.572982Z","shell.execute_reply":"2021-08-23T23:35:04.571864Z","shell.execute_reply.started":"2021-08-23T23:35:04.474766Z"},"papermill":{"duration":0.138541,"end_time":"2021-07-16T17:41:54.471374","exception":false,"start_time":"2021-07-16T17:41:54.332833","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Passage from \"Quincas borba\"\n","print(ranked_search(10, tf_idf, files_with_index, \n","                    \"Ao vencido, ódio ou compaixão; ao vencedor, as batatas\"))\n","print(ranked_search(10, tf_idf, files_with_index, \"Quincas borba\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.574528Z","iopub.status.busy":"2021-08-23T23:35:04.574204Z","iopub.status.idle":"2021-08-23T23:35:04.666949Z","shell.execute_reply":"2021-08-23T23:35:04.665834Z","shell.execute_reply.started":"2021-08-23T23:35:04.574497Z"},"papermill":{"duration":0.138959,"end_time":"2021-07-16T17:41:54.656303","exception":false,"start_time":"2021-07-16T17:41:54.517344","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Passage from \"Dom Casmurro\"\n","print(ranked_search(10, tf_idf, files_with_index, \n","                    \"Capitu, apesar daqueles olhos que o diabo lhe deu\"))\n","print(ranked_search(10, tf_idf, files_with_index, \"Capitu\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.670754Z","iopub.status.busy":"2021-08-23T23:35:04.670442Z","iopub.status.idle":"2021-08-23T23:35:04.794966Z","shell.execute_reply":"2021-08-23T23:35:04.793961Z","shell.execute_reply.started":"2021-08-23T23:35:04.670726Z"},"papermill":{"duration":0.1741,"end_time":"2021-07-16T17:41:54.876703","exception":false,"start_time":"2021-07-16T17:41:54.702603","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Passage from \"Memórias Póstumas de Brás Cubas\"\n","print(ranked_search(10, tf_idf, files_with_index, \n","                    \"Sandice criar amor às casas alheias, de modo que, \\\n","                    apenas senhora de uma, dificilmente lha farão despejar\"))\n","print(ranked_search(10, tf_idf, files_with_index, \"Sandice\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.796498Z","iopub.status.busy":"2021-08-23T23:35:04.796187Z","iopub.status.idle":"2021-08-23T23:35:04.834682Z","shell.execute_reply":"2021-08-23T23:35:04.833511Z","shell.execute_reply.started":"2021-08-23T23:35:04.796469Z"},"papermill":{"duration":0.046262,"end_time":"2021-07-16T17:41:55.06484","exception":false,"start_time":"2021-07-16T17:41:55.018578","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["## Here we reproduce the previous query with common words, \n","## but now we have a score to sort results.\n","print(ranked_search(10, tf_idf, files_with_index, \"árvore rua\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.046211,"end_time":"2021-07-16T17:41:55.253069","exception":false,"start_time":"2021-07-16T17:41:55.206858","status":"completed"},"tags":[]},"source":["# 3. Evaluating IR Systems\n","\n","In this section we'll discuss how to evaluate an Information Retrieval System. To assess our design decisions (kind of data structure, preprocessing steps, type of term weigthing, etc) we need to set up a benchmark.\n","\n","There are several available benchmarks over the Internet. Probably the most important IR dataset nowadays is [MS Marco Document retrieval dataset](https://microsoft.github.io/msmarco/). It contains more than 3 million documents and 300k queries. Besides documents and queries a IR benchmark must be a relevance mapping between them. This way we can evaluate if a document returned by our system should be returned or not according to this mapping.\n","\n","There are several metrics to evaluate IR systems. The evaluation process consists of \"firing\" a set of queries \"against\" the IR System and compare the returned documents with the answers annotated in relevance mapping. Some metrics use the orders of returned documents, but others don't. In some metrics we define a cut in the number of documents returned (i.e. top 10 documents only). A extensive list of metrics can be found [here](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)).\n","\n","In this tutorial we'll use [MRR@10 (Mean Reciprocal Rank)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), a metric which takes into account only the position of the first relevant document returned into the first 10 documents by each query. MS Marco benchmark also uses this metric, but it is calculated with 100 first returned results.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The code above will receive a boolean relevance results vector and return the MRR@10."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:04.836456Z","iopub.status.busy":"2021-08-23T23:35:04.836122Z","iopub.status.idle":"2021-08-23T23:35:04.848492Z","shell.execute_reply":"2021-08-23T23:35:04.847489Z","shell.execute_reply.started":"2021-08-23T23:35:04.836425Z"},"trusted":true},"outputs":[],"source":["##Source: https://gist.github.com/bwhite/3726239\n","def mean_reciprocal_rank(bool_results, k=10):\n","    \"\"\"Score is reciprocal of the rank of the first relevant item\n","    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n","    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n","    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n","    >>> mean_reciprocal_rank(rs)\n","    0.61111111111111105\n","\n","    Args:\n","        rs: Iterator of relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        Mean reciprocal rank\n","    \"\"\"\n","    bool_results = (np.atleast_1d(r[:k]).nonzero()[0] for r in bool_results)\n","    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in bool_results])\n","\n","mean_reciprocal_rank([[0, 0, 1], [0, 1, 0], [1, 0, 0]])"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-08-23T23:35:04.850181Z","iopub.status.busy":"2021-08-23T23:35:04.849891Z","iopub.status.idle":"2021-08-23T23:35:11.944486Z","shell.execute_reply":"2021-08-23T23:35:11.943326Z","shell.execute_reply.started":"2021-08-23T23:35:04.850145Z"},"papermill":{"duration":9.234171,"end_time":"2021-07-16T17:42:04.534024","exception":false,"start_time":"2021-07-16T17:41:55.299853","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["!pip install rank_bm25\n","from rank_bm25 import BM25Okapi"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3.1 Load and process CISI dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instead of using a huge dataset, for didatic purposes we'll use a much smaller one, known as [CISI collection](http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/), from Glasgow Univesity. The code below loads the original dataset into dictionaries to easy access."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-08-23T23:35:11.947721Z","iopub.status.busy":"2021-08-23T23:35:11.947416Z","iopub.status.idle":"2021-08-23T23:35:12.140928Z","shell.execute_reply":"2021-08-23T23:35:12.139881Z","shell.execute_reply.started":"2021-08-23T23:35:11.947691Z"},"trusted":true},"outputs":[],"source":["### Processing DOCUMENTS\n","doc_set = {}\n","doc_id = \"\"\n","doc_text = \"\"\n","with open('/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.ALL') as f:\n","    lines = \"\"\n","    for l in f.readlines():\n","        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n","    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n","doc_count = 0\n","for l in lines:\n","    if l.startswith(\".I\"):\n","        doc_id = int(l.split(\" \")[1].strip())-1\n","    elif l.startswith(\".X\"):\n","        doc_set[doc_id] = doc_text.lstrip(\" \")\n","        doc_id = \"\"\n","        doc_text = \"\"\n","    else:\n","        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.    \n","\n","        \n","### Processing QUERIES\n","with open('/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.QRY') as f:\n","    lines = \"\"\n","    for l in f.readlines():\n","        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n","    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n","    \n","qry_set = {}\n","qry_id = \"\"\n","for l in lines:\n","    if l.startswith(\".I\"):\n","        qry_id = int(l.split(\" \")[1].strip()) -1\n","    elif l.startswith(\".W\"):\n","        qry_set[qry_id] = l.strip()[3:]\n","        qry_id = \"\"\n","\n","### Processing QRELS\n","rel_set = {}\n","with open('/kaggle/input/cisi-a-dataset-for-information-retrieval/CISI.REL') as f:\n","    for l in f.readlines():\n","        qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]) -1\n","        doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])-1\n","        if qry_id in rel_set:\n","            rel_set[qry_id].append(doc_id)\n","        else:\n","            rel_set[qry_id] = []\n","            rel_set[qry_id].append(doc_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:12.144712Z","iopub.status.busy":"2021-08-23T23:35:12.144412Z","iopub.status.idle":"2021-08-23T23:35:12.154656Z","shell.execute_reply":"2021-08-23T23:35:12.153373Z","shell.execute_reply.started":"2021-08-23T23:35:12.144685Z"},"trusted":true},"outputs":[],"source":["## Here we check some statistics and info of CISI dataset\n","\n","print('Read %s documents, %s queries and %s mappings from CISI dataset' % \n","      (len(doc_set), len(qry_set), len(rel_set)))\n","\n","number_of_rel_docs = [len(value) for key, value in rel_set.items()]\n","print('Average %.2f and %d min number of relevant documents by query ' % \n","      (np.mean(number_of_rel_docs), np.min(number_of_rel_docs)))\n","\n","print('Queries without relevant documents: ', \n","      np.setdiff1d(list(qry_set.keys()),list(rel_set.keys())))"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-07-16T22:49:46.370422Z","iopub.status.busy":"2021-07-16T22:49:46.370047Z","iopub.status.idle":"2021-07-16T22:49:46.376003Z","shell.execute_reply":"2021-07-16T22:49:46.37497Z","shell.execute_reply.started":"2021-07-16T22:49:46.370393Z"}},"source":["Below there's a sample of a pair query and a document relevant to it in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:12.159675Z","iopub.status.busy":"2021-08-23T23:35:12.159378Z","iopub.status.idle":"2021-08-23T23:35:12.170919Z","shell.execute_reply":"2021-08-23T23:35:12.169674Z","shell.execute_reply.started":"2021-08-23T23:35:12.159648Z"},"trusted":true},"outputs":[],"source":["random.seed(42)\n","idx = random.sample(rel_set.keys(),1)[0]\n","\n","print('Query ID %s ==>' % idx, qry_set[idx])\n","rel_docs = rel_set[idx]\n","print('Documents relevants to Query ID %s' % idx, rel_docs)\n","sample_document_idx = random.sample(rel_docs,1)[0]\n","print('Document ID %s ==>' % sample_document_idx, doc_set[sample_document_idx])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3.2 Index CISI dataset using BM25"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To evaluate our benchmark, instead of using the Tf-Idf model we build previously, we'll use an API which implements a rank function called [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25). It's the standard function built in [Apache Lucene](https://lucene.apache.org/), [Elastic](https://www.elastic.co/) and [Apache Solr](https://solr.apache.org/), leading solutions for indexing and searching documents.\n","\n","BM25 is very similar to Tf-Idf concept we discussed earlier in this tutorial."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In the code below we index each document from CISI without any preprocessing and get scores for one random query."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:35:12.173091Z","iopub.status.busy":"2021-08-23T23:35:12.172764Z","iopub.status.idle":"2021-08-23T23:35:12.350717Z","shell.execute_reply":"2021-08-23T23:35:12.349629Z","shell.execute_reply.started":"2021-08-23T23:35:12.173062Z"},"trusted":true},"outputs":[],"source":["query = qry_set[idx] #get query text\n","rel_docs = rel_set[idx] #get relevant documents\n","\n","# Index all documents using BM25\n","corpus = list(doc_set.values())\n","tokenized_corpus = [doc.split(\" \") for doc in corpus]\n","bm25 = BM25Okapi(tokenized_corpus)\n","\n","# Process query and get scores for each indexed document using BM25\n","tokenized_query = query.split(\" \")\n","print('Query ==> ', query, '\\nRelevant documents IDs: ==> ', rel_docs)\n","scores = bm25.get_scores(tokenized_query)\n","print(scores, len(scores), len(doc_set))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Finally we sort documents by score, compare with hand annotated relevant documents from dataset and create a boolean mask of the results. With this boolean array we can calculate MRR@10."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:38:15.346234Z","iopub.status.busy":"2021-08-23T23:38:15.345828Z","iopub.status.idle":"2021-08-23T23:38:15.354893Z","shell.execute_reply":"2021-08-23T23:38:15.353879Z","shell.execute_reply.started":"2021-08-23T23:38:15.346196Z"},"trusted":true},"outputs":[],"source":["## Argsort gives the indexes of values in increasing order, so we input with the negative values of scores\n","most_relevant_documents = np.argsort(-scores)\n","\n","print(most_relevant_documents[:20]) # printing first 20 most relevant results\n","\n","## Mask relevant documents with 0's and 1's according to query <-> document annotation\n","masked_relevance_results = np.zeros(most_relevant_documents.shape)\n","masked_relevance_results[rel_docs] = 1\n","sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n","\n","print(sorted_masked_relevance_results[:20]) #printing first 20 results: 1 is relevant 0 isn't\n","\n","# Calculate MRR@10\n","print(mean_reciprocal_rank([sorted_masked_relevance_results]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we're ready to reproduce scores through all queries in dataset. First we'll create a function to return the masked results."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:43:14.684363Z","iopub.status.busy":"2021-08-23T23:43:14.683834Z","iopub.status.idle":"2021-08-23T23:43:20.080623Z","shell.execute_reply":"2021-08-23T23:43:20.079606Z","shell.execute_reply.started":"2021-08-23T23:43:14.684317Z"},"trusted":true},"outputs":[],"source":["def results_from_query(qry_id, bm25):\n","    \"\"\"Return an ordered array of relevant documents returned by query_id\n","\n","    Args:\n","        qry_id (int): id of query on dataset\n","        bm25 (object): indexed corpus\n","\n","    Returns:\n","        boolean sorted relevance array of documents\n","    \"\"\"    \n","    query = qry_set[qry_id]\n","    rel_docs = []\n","    if qry_id in rel_set:\n","        rel_docs = rel_set[qry_id]\n","    tokenized_query = query.split(\" \")\n","    scores = bm25.get_scores(tokenized_query)\n","    most_relevant_documents = np.argsort(-scores)\n","    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n","  \n","    masked_relevance_results[rel_docs] = 1\n","    sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n","    \n","    return sorted_masked_relevance_results\n","\n","\n","results = [results_from_query(qry_id, bm25) for qry_id in list(qry_set.keys())]\n","print('MRR@10 %.4f' % mean_reciprocal_rank(results))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3.3 Trying to improve results\n","\n","In this section we'll try to improve results through preprocessing our corpus and query using stemming, lowercase and removing stop words."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:45:07.154952Z","iopub.status.busy":"2021-08-23T23:45:07.154491Z","iopub.status.idle":"2021-08-23T23:45:07.161107Z","shell.execute_reply":"2021-08-23T23:45:07.160198Z","shell.execute_reply.started":"2021-08-23T23:45:07.154921Z"},"trusted":true},"outputs":[],"source":["# Instaciate objects from NLTK\n","stemmer = nltk.stem.PorterStemmer()\n","stop_words = nltk.corpus.stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:47:52.416366Z","iopub.status.busy":"2021-08-23T23:47:52.41596Z","iopub.status.idle":"2021-08-23T23:47:52.423278Z","shell.execute_reply":"2021-08-23T23:47:52.422173Z","shell.execute_reply.started":"2021-08-23T23:47:52.416335Z"},"trusted":true},"outputs":[],"source":["def preprocess_string(txt, remove_stop=True, do_stem=True, to_lower=True):\n","    \"\"\"\n","    Return a preprocessed tokenized text.\n","    \n","    Args:\n","        txt (str): original text to process\n","        remove_stop (boolean): to remove or not stop words (common words)\n","        do_stem (boolean): to do or not stemming (suffixes and prefixes removal)\n","        to_lower (boolean): remove or not capital letters.\n","        \n","    Returns:\n","        Return a preprocessed tokenized text.\n","    \"\"\"      \n","    if to_lower:\n","        txt = txt.lower()\n","    tokens = nltk.tokenize.word_tokenize(txt)\n","    \n","    if remove_stop:\n","        tokens = [tk for tk in tokens if tk not in stop_words]\n","    if do_stem:\n","        tokens = [stemmer.stem(tk) for tk in tokens]\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-08-23T23:49:01.744145Z","iopub.status.busy":"2021-08-23T23:49:01.743796Z","iopub.status.idle":"2021-08-23T23:49:11.864965Z","shell.execute_reply":"2021-08-23T23:49:11.864058Z","shell.execute_reply.started":"2021-08-23T23:49:01.744116Z"},"trusted":true},"outputs":[],"source":["corpus = list(doc_set.values())\n","# You may experiment with this trying to improve MRR@10\n","remove_stop = True\n","do_stem = True\n","to_lower = True\n","\n","tokenized_corpus = [preprocess_string(doc, remove_stop, do_stem, to_lower) for doc in corpus]\n","\n","bm25 = BM25Okapi(tokenized_corpus)\n","\n","def results_from_query_new(qry_id, bm25):\n","    query = qry_set[qry_id]\n","    rel_docs = []\n","    if qry_id in rel_set:\n","        rel_docs = rel_set[qry_id]\n","    tokenized_query = preprocess_string(query, remove_stop, do_stem, to_lower)\n","    scores = bm25.get_scores(tokenized_query)\n","    most_relevant_documents = np.argsort(-scores)\n","    masked_relevance_results = np.zeros(most_relevant_documents.shape)\n","    masked_relevance_results[rel_docs] = 1\n","    sorted_masked_relevance_results = np.take(masked_relevance_results, most_relevant_documents)\n","    return sorted_masked_relevance_results\n","\n","\n","results = [results_from_query_new(qry_id, bm25) for qry_id in list(qry_set.keys())]\n","print('MRR@10 %.4f' % mean_reciprocal_rank(results))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see, a huge improvement (~35%) in MRR@10 doing this 3 preprocessing steps !!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
